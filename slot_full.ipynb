{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisemoelgaard/Dataproject/blob/main/slot_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"OleSahlholdt\"\n",
        "!git config --global user.email \"o.sahlholdt@gmail.com\"\n",
        "!git config --global user.password \"\"\n",
        "Token = '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'\n",
        "username = 'louisemoelgaard'\n",
        "repo = 'deep-learning-project'\n",
        "\n",
        "!git clone https://{Token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tWSLK4hZOi6",
        "outputId": "83c87261-e19f-428e-cbf2-f873b26003b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning-project'...\n",
            "remote: Enumerating objects: 68825, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 68825 (delta 39), reused 31 (delta 12), pack-reused 68750\u001b[K\n",
            "Receiving objects: 100% (68825/68825), 9.54 GiB | 17.63 MiB/s, done.\n",
            "Resolving deltas: 100% (284/284), done.\n",
            "Checking out files: 100% (45930/45930), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deep-learning-project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb7STbfaoq9",
        "outputId": "15546563-46b6-41a2-894e-24563e2ddb73"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-learning-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cd4VYQOqpsTQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as mpl_color_map\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4_sxnERZMav"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "y02InYzsZMaz"
      },
      "outputs": [],
      "source": [
        "class AVADataSet(Dataset):\n",
        "    def __init__(self, csv, img_dir, transform, target_var):\n",
        "        self.img_labels = csv\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_var = target_var\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        trans = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
        "        img_name = str(self.img_labels.loc[idx, 'image']) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        image = read_image(img_path)\n",
        "        image = trans(image).to(torch.float64)/255\n",
        "        if image.size(0) != 3:\n",
        "            image = image[0:3]\n",
        "\n",
        "        image = self.transform(image)\n",
        "        # get label form csv\n",
        "        label = self.img_labels.loc[idx, self.target_var]\n",
        "        return image, label\n",
        "    def get_name(self, idx):\n",
        "        img_name = str(self.img_labels.loc[idx, 'image']) + '.jpg'\n",
        "        label = self.img_labels.loc[idx, self.target_var]\n",
        "        return img_name, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWYkHMgKZMa0"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RdCbFrHCoBc1"
      },
      "outputs": [],
      "source": [
        "AVA = True\n",
        "batch_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBm0w7NoKx3"
      },
      "outputs": [],
      "source": [
        "if AVA == False:\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize([224, 224]),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    dataset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=True, download=True, transform=transform\n",
        "    )\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=False, download=True, transform=transform\n",
        "    )\n",
        "    val_size = 5000\n",
        "    train_size = len(dataset) - val_size\n",
        "    trainset, valset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        valset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BZ5JfW8tVKcf"
      },
      "outputs": [],
      "source": [
        "if AVA == True:\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            #transforms.ToTensor(),\n",
        "            transforms.Resize([224, 224]),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "    data = pd.read_csv(\"datasets/data_class.csv\")\n",
        "\n",
        "    train_data = data.sample(frac=0.75)\n",
        "    rest = data.drop(train_data.index)\n",
        "    val_data = rest.sample(frac=0.5)\n",
        "    test_data = rest.drop(val_data.index)\n",
        "\n",
        "    train_dataset = AVADataSet(\n",
        "        train_data.reset_index(),\n",
        "        \"dataset_class\",\n",
        "        transform=transform,\n",
        "        target_var=\"2class\",\n",
        "    )\n",
        "    val_dataset = AVADataSet(\n",
        "        val_data.reset_index(),\n",
        "        \"dataset_class\",\n",
        "        transform=transform,\n",
        "        target_var=\"2class\",\n",
        "    )\n",
        "    test_dataset = AVADataSet(\n",
        "        test_data.reset_index(),\n",
        "        \"dataset_class\",\n",
        "        transform=transform,\n",
        "        target_var=\"2class\",\n",
        "    )\n",
        "\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RQoagNzupvvF"
      },
      "outputs": [],
      "source": [
        "class SlotAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    This function is heavily build on the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dim, iters=3, eps=1e-8, power=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.iters = iters\n",
        "        self.eps = eps\n",
        "        self.scale = dim**-0.5\n",
        "        self.power = power\n",
        "\n",
        "        mu = nn.Parameter(torch.randn(1, 1, dim)).expand(1, self.num_classes, -1)\n",
        "        sigma = abs(nn.Parameter(torch.randn(1, 1, dim))).expand(\n",
        "            1, self.num_classes, -1\n",
        "        )\n",
        "        self.initial_slots = nn.Parameter(torch.normal(mu, sigma))\n",
        "\n",
        "        self.FC1 = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "        self.FC2 = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(dim, dim)\n",
        "\n",
        "    def dot_attention(self, slots, inputs):\n",
        "        dots = torch.einsum(\"bid,bjd->bij\", slots, inputs) * self.scale\n",
        "        div = torch.div(\n",
        "            dots, dots.sum(2).expand_as(dots.permute([2, 0, 1])).permute([1, 2, 0])\n",
        "        )\n",
        "        mult = dots.sum(2).sum(1).expand_as(dots.permute([1, 2, 0])).permute([2, 0, 1])\n",
        "        return div * mult\n",
        "\n",
        "    def save_slot(self, slots):\n",
        "        out = (slots - slots.min()) / (slots.max() - slots.min()) * 255.0\n",
        "        out = out.reshape(\n",
        "            out.shape[:1] + (int(out.size(1) ** 0.5), int(out.size(1) ** 0.5))\n",
        "        )\n",
        "        out = (out.cpu().detach().numpy()).astype(np.uint8)\n",
        "        for i, image in enumerate(out):\n",
        "            image = Image.fromarray(image, mode=\"L\")\n",
        "            image.save(f\"slot_{i:d}.png\")\n",
        "\n",
        "    def forward(self, inputs, inputs_x):\n",
        "        batch, channels, elements = inputs.shape\n",
        "        slots = self.initial_slots.expand(batch, -1, -1)\n",
        "        inputs = self.FC1(inputs)\n",
        "\n",
        "        for _ in range(self.iters):\n",
        "            prev_slots = slots\n",
        "\n",
        "            #slots = self.FC2(slots)\n",
        "\n",
        "            dots = self.dot_attention(slots, inputs)\n",
        "            attn = torch.sigmoid(dots)\n",
        "\n",
        "            updates = torch.einsum(\"bjd,bij->bid\", inputs_x, attn)\n",
        "            updates = updates / inputs_x.size(2)\n",
        "\n",
        "            self.gru.flatten_parameters()\n",
        "            slots, _ = self.gru(\n",
        "                updates.reshape(1, -1, elements), prev_slots.reshape(1, -1, elements)\n",
        "            )\n",
        "\n",
        "            slots = slots.reshape(batch, -1, elements)\n",
        "\n",
        "        slots_vis = attn.clone()\n",
        "        slots_vis = slots_vis[0]\n",
        "\n",
        "        self.save_slot(slots_vis)\n",
        "\n",
        "        attn_relu = torch.relu(attn)\n",
        "        loss = torch.sum(attn_relu) / attn.size(0) / attn.size(1) / attn.size(2)\n",
        "        loss = torch.pow(loss, self.power)\n",
        "        output = torch.sum(updates, dim=2, keepdim=False)\n",
        "\n",
        "        return output, loss\n",
        "\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This function is the same as the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nfeats=64, temperature=10000):\n",
        "        super().__init__()\n",
        "        self.nfeats = nfeats\n",
        "        self.temperature = temperature\n",
        "        self.scale = 2 * math.pi\n",
        "\n",
        "    def forward(self, tensor_list):\n",
        "        x = tensor_list\n",
        "        b, c, h, w = x.shape\n",
        "        mask = torch.zeros((b, h, w), dtype=torch.bool, device=x.device)\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "\n",
        "        eps = 1e-6\n",
        "        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.nfeats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.nfeats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack(\n",
        "            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n",
        "        ).flatten(3)\n",
        "        pos_y = torch.stack(\n",
        "            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n",
        "        ).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos.to(x.dtype)\n",
        "\n",
        "\n",
        "class Identical(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identical, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class SlotModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This function is heavily build on the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, hidden_dim, input_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.conv1x1 = nn.Conv2d(512, hidden_dim, kernel_size=(1, 1), stride=(1, 1))\n",
        "        N_steps = hidden_dim // 2\n",
        "        self.position_emb = PositionEmbeddingSine(N_steps)\n",
        "        self.slot = SlotAttention(num_classes, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), *self.input_size)\n",
        "        x = self.conv1x1(x)\n",
        "        x = torch.relu(x)\n",
        "        pe = self.position_emb(x)\n",
        "        x_pe = x + pe\n",
        "\n",
        "        batch, channel, r, c = x.shape\n",
        "        x = x.reshape((batch, channel, -1)).permute((0, 2, 1))\n",
        "\n",
        "        x_pe = x_pe.reshape((batch, channel, -1)).permute((0, 2, 1))\n",
        "        x, attn_loss = self.slot(x_pe, x)\n",
        "\n",
        "        return x, attn_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WTs-JiwNawBw"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, slot, lambda_value):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    for images, targets in dataloader:\n",
        "        images, targets = images.to(device, dtype=torch.float), targets.to(device)\n",
        "        if slot:\n",
        "            output, attn_loss = model(images)\n",
        "            n_loss = criterion(output, targets)\n",
        "            loss = n_loss + lambda_value * attn_loss\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(output, targets)\n",
        "\n",
        "        topv, topi = torch.topk(output, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(topi.view(topi.size(0)) == targets)\n",
        "\n",
        "    loss = running_loss / len(dataloader.dataset)\n",
        "    acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ICrF62rwpv18"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloaders, nEpochs, optimizer, device, slot, lambda_value=None):\n",
        "    l_acc = {\"train\": [], \"val\": []}\n",
        "    l_loss = {\"train\": [], \"val\": []}\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"Initial performance\")\n",
        "    for phase in [\"train\", \"val\"]:\n",
        "        init_loss, init_acc = evaluate(\n",
        "            model, dataloaders[phase], criterion, slot, lambda_value\n",
        "        )\n",
        "        l_acc[phase].append(init_acc.item())\n",
        "        l_loss[phase].append(init_loss)\n",
        "\n",
        "        print(f\"{phase}: Loss: {init_loss}, Accuracy: {init_acc}\")\n",
        "\n",
        "    print(\"Start training\")\n",
        "    best_loss = 100\n",
        "    for epoch in range(nEpochs):\n",
        "        print(\"Epoch: \", epoch)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for images, targets in dataloaders[phase]:\n",
        "                if phase == \"train\":\n",
        "                    torch.set_grad_enabled(True)\n",
        "                    model.train()\n",
        "\n",
        "                else:\n",
        "                    torch.set_grad_enabled(False)\n",
        "                    model.eval()\n",
        "\n",
        "                images = images.float()\n",
        "                images = images.to(device)\n",
        "                targets = targets.to(device)\n",
        "                if slot:\n",
        "                    output, attn_loss = model(images)\n",
        "                    n_loss = criterion(output, targets)\n",
        "                    loss = n_loss + lambda_value * attn_loss\n",
        "                else:\n",
        "                    output = model(images)\n",
        "                    loss = criterion(output, targets)\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                topv, topi = torch.topk(output, 1)\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                running_corrects += torch.sum(topi.view(topi.size(0)) == targets)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            l_loss[phase].append(epoch_loss)\n",
        "            l_acc[phase].append(epoch_acc.item())\n",
        "\n",
        "            if best_loss > epoch_loss and phase == \"val\":\n",
        "                best_loss = epoch_loss\n",
        "                end_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print(f\"{phase}: Loss: {epoch_loss}, Accuracy: {epoch_acc}\")\n",
        "\n",
        "    model.load_state_dict(end_model)\n",
        "    return model, l_loss, l_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef1b08f63c4348ebbcf7fb2f23609283",
            "3833d138c95947569227792f120a289b",
            "03bad8c3a788494a90d240fcd3bc5b2c",
            "6273d76ae74444c58f5759d5d37a24d1",
            "4618dca4c13a435faf942f9309ceb8fd",
            "4a5bc00a131545afa17901dc414bb6cb",
            "8a8fb24d27284a868f43456feabf5d20",
            "c77e85fdd12846a5802f168c37f829e4",
            "469cc617bbe24385bf4cc666499e380d",
            "6760d75673f24df7a582bea1fdeae98f",
            "cc4e48b4982b4db2bbac1539582ccc62"
          ]
        },
        "id": "7Tef7oes6XUq",
        "outputId": "0304bdda-aae2-46c3-8255-026cbe42242e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef1b08f63c4348ebbcf7fb2f23609283"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters trained:\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.bn1.bias\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.bn2.bias\n",
            "layer4.0.downsample.0.weight\n",
            "layer4.0.downsample.1.weight\n",
            "layer4.0.downsample.1.bias\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.bn1.bias\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "layer4.1.bn2.bias\n",
            "fc.conv1x1.weight\n",
            "fc.conv1x1.bias\n",
            "fc.slot.initial_slots\n",
            "fc.slot.FC1.0.weight\n",
            "fc.slot.FC1.0.bias\n",
            "fc.slot.FC1.2.weight\n",
            "fc.slot.FC1.2.bias\n",
            "fc.slot.FC1.4.weight\n",
            "fc.slot.FC1.4.bias\n",
            "fc.slot.FC2.0.weight\n",
            "fc.slot.FC2.0.bias\n",
            "fc.slot.FC2.2.weight\n",
            "fc.slot.FC2.2.bias\n",
            "fc.slot.FC2.4.weight\n",
            "fc.slot.FC2.4.bias\n",
            "fc.slot.gru.weight_ih_l0\n",
            "fc.slot.gru.weight_hh_l0\n",
            "fc.slot.gru.bias_ih_l0\n",
            "fc.slot.gru.bias_hh_l0\n",
            "Initial performance\n",
            "train: Loss: 0.9482078274649648, Accuracy: 0.48138359665427505\n",
            "val: Loss: 0.9444570555924622, Accuracy: 0.48849773440223077\n",
            "Start training\n",
            "Epoch:  0\n",
            "train: Loss: 0.7539752673800882, Accuracy: 0.513650092936803\n",
            "val: Loss: 0.6932889898507261, Accuracy: 0.5714534681073545\n",
            "Epoch:  1\n",
            "train: Loss: 0.6657384880074061, Accuracy: 0.6231122211895911\n",
            "val: Loss: 0.5162197106098123, Accuracy: 0.7450331125827815\n",
            "Epoch:  2\n",
            "train: Loss: 0.4565968885117636, Accuracy: 0.7897885687732341\n",
            "val: Loss: 0.44478702437133893, Accuracy: 0.7955733705123736\n",
            "Epoch:  3\n",
            "train: Loss: 0.30833151550153376, Accuracy: 0.8698594330855018\n",
            "val: Loss: 0.47968725596790573, Accuracy: 0.7938306029975601\n",
            "Epoch:  4\n",
            "train: Loss: 0.12720031656851113, Accuracy: 0.9537639405204461\n",
            "val: Loss: 0.6744274077057298, Accuracy: 0.7814569536423841\n",
            "Epoch:  5\n",
            "train: Loss: 0.05882763427751032, Accuracy: 0.9803671003717471\n",
            "val: Loss: 0.92812577449889, Accuracy: 0.7819797838968282\n"
          ]
        }
      ],
      "source": [
        "if AVA == False:\n",
        "    nclass = 10\n",
        "else:\n",
        "    nclass = 2\n",
        "\n",
        "dataloaders = {\"train\": trainloader, \"val\": valloader}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "num_fts = resnet.fc.in_features\n",
        "\n",
        "for name, param in resnet.named_parameters():\n",
        "    if \"layer4\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "resnet.avgpool = Identical()\n",
        "resnet.fc = SlotModel(num_classes=nclass, hidden_dim=128, input_size=(512,7,7))\n",
        "resnet.to(device)\n",
        "\n",
        "print('Parameters trained:')\n",
        "params_to_update = []\n",
        "for name, param in resnet.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "        params_to_update.append(param)\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=1e-4)        \n",
        "model, h_loss, h_accuracy = train(model=resnet, dataloaders=dataloaders, nEpochs=6, optimizer=optimizer, device=device, slot=True, lambda_value=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()"
      ],
      "metadata": {
        "id": "qZC9A_dTv46j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "lc35ndqZGikb",
        "outputId": "4a5d739d-d568-4d68-e038-62d30bfeadbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3568227696208967, tensor(0.8596, device='cuda:0', dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "evaluate(model, testloader, criterion, slot=True, lambda_value=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6Bj-ksv4Oqp2"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), f'Slot_AVA_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "04Mb--aPHJVU"
      },
      "outputs": [],
      "source": [
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "    This function is the same as the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap * 255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap * 255).astype(np.uint8))\n",
        "    heatmap_on_image = Image.new(\"RGBA\", (224,224))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert(\"RGBA\"))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "3reVc3Z1Wqcn"
      },
      "outputs": [],
      "source": [
        "def eval_attn(model, imageid, device):\n",
        "    trans = transforms.Compose([transforms.Resize([224, 224]),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    model.eval()\n",
        "    image = read_image(f'dataset_class/{imageid}')\n",
        "    image = image.to(torch.float64)/255\n",
        "    image = trans(image)\n",
        "    image = image.float()\n",
        "    image = image.to(device)\n",
        "    out, att = model(image.view(1,3,224,224))\n",
        "    print(f'prediction: {out.cpu().numpy()[0]}')\n",
        "    for id in range(2):\n",
        "        image_raw = Image.open(f'dataset_class/{imageid}').convert('RGB')\n",
        "        image_raw = image_raw.resize((224,224), resample=Image.BILINEAR)\n",
        "        slot_image = np.array(Image.open(f'slot_{id}.png').resize((224,224), resample=Image.BILINEAR), dtype=np.uint8)\n",
        "        heatmap_only, heatmap_on_image = apply_colormap_on_image(image_raw, slot_image, 'jet')\n",
        "        heatmap_on_image.save(f'slot_mask_{id}.png')   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "j9uva7Thajil",
        "outputId": "7b41e943-b836-4d52-f4e8-4e68ac872690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [1.0466216 3.8938477]\n"
          ]
        }
      ],
      "source": [
        "#test_dataset\n",
        "eval_attn(model, '923886.jpg', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha6HJVDh29jR"
      },
      "outputs": [],
      "source": [
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000,1100):\n",
        "  k = test_dataset.get_name(i)\n",
        "  if k[1] == 1:\n",
        "    print(k)"
      ],
      "metadata": {
        "id": "hRJOluhwuSVu",
        "outputId": "54b130ba-73d7-4e26-d15e-e98b99518876",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('81312.jpg', 1)\n",
            "('111809.jpg', 1)\n",
            "('111497.jpg', 1)\n",
            "('110172.jpg', 1)\n",
            "('111842.jpg', 1)\n",
            "('586892.jpg', 1)\n",
            "('587832.jpg', 1)\n",
            "('586466.jpg', 1)\n",
            "('588041.jpg', 1)\n",
            "('588039.jpg', 1)\n",
            "('587809.jpg', 1)\n",
            "('587715.jpg', 1)\n",
            "('588010.jpg', 1)\n",
            "('587137.jpg', 1)\n",
            "('585335.jpg', 1)\n",
            "('63472.jpg', 1)\n",
            "('63717.jpg', 1)\n",
            "('63394.jpg', 1)\n",
            "('63335.jpg', 1)\n",
            "('63371.jpg', 1)\n",
            "('63432.jpg', 1)\n",
            "('63657.jpg', 1)\n",
            "('63215.jpg', 1)\n",
            "('33109.jpg', 1)\n",
            "('32516.jpg', 1)\n",
            "('33045.jpg', 1)\n",
            "('12879.jpg', 1)\n",
            "('13039.jpg', 1)\n",
            "('548063.jpg', 1)\n",
            "('551405.jpg', 1)\n",
            "('549350.jpg', 1)\n",
            "('551207.jpg', 1)\n",
            "('603375.jpg', 1)\n",
            "('603691.jpg', 1)\n",
            "('604020.jpg', 1)\n",
            "('603987.jpg', 1)\n",
            "('602381.jpg', 1)\n",
            "('603806.jpg', 1)\n",
            "('603640.jpg', 1)\n",
            "('882575.jpg', 1)\n",
            "('882745.jpg', 1)\n",
            "('874935.jpg', 1)\n",
            "('874756.jpg', 1)\n",
            "('527388.jpg', 1)\n",
            "('524480.jpg', 1)\n",
            "('525533.jpg', 1)\n",
            "('527461.jpg', 1)\n",
            "('526534.jpg', 1)\n",
            "('527496.jpg', 1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef1b08f63c4348ebbcf7fb2f23609283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3833d138c95947569227792f120a289b",
              "IPY_MODEL_03bad8c3a788494a90d240fcd3bc5b2c",
              "IPY_MODEL_6273d76ae74444c58f5759d5d37a24d1"
            ],
            "layout": "IPY_MODEL_4618dca4c13a435faf942f9309ceb8fd"
          }
        },
        "3833d138c95947569227792f120a289b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a5bc00a131545afa17901dc414bb6cb",
            "placeholder": "​",
            "style": "IPY_MODEL_8a8fb24d27284a868f43456feabf5d20",
            "value": "100%"
          }
        },
        "03bad8c3a788494a90d240fcd3bc5b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77e85fdd12846a5802f168c37f829e4",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_469cc617bbe24385bf4cc666499e380d",
            "value": 46830571
          }
        },
        "6273d76ae74444c58f5759d5d37a24d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6760d75673f24df7a582bea1fdeae98f",
            "placeholder": "​",
            "style": "IPY_MODEL_cc4e48b4982b4db2bbac1539582ccc62",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 79.8MB/s]"
          }
        },
        "4618dca4c13a435faf942f9309ceb8fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5bc00a131545afa17901dc414bb6cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a8fb24d27284a868f43456feabf5d20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c77e85fdd12846a5802f168c37f829e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "469cc617bbe24385bf4cc666499e380d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6760d75673f24df7a582bea1fdeae98f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc4e48b4982b4db2bbac1539582ccc62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}