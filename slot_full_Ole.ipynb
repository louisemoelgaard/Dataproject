{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisemoelgaard/Dataproject/blob/main/slot_full_Ole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.name \"OleSahlholdt\"\n",
        "!git config --global user.email \"o.sahlholdt@gmail.com\"\n",
        "!git config --global user.password \"\"\n",
        "Token = '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'\n",
        "username = 'louisemoelgaard'\n",
        "repo = 'deep-learning-project'\n",
        "\n",
        "!git clone https://{Token}@github.com/{username}/{repo}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tWSLK4hZOi6",
        "outputId": "40855a8d-ff14-4c15-ac18-bb705efe4eed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-learning-project'...\n",
            "remote: Enumerating objects: 68831, done.\u001b[K\n",
            "remote: Counting objects: 100% (81/81), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 68831 (delta 43), reused 31 (delta 12), pack-reused 68750\u001b[K\n",
            "Receiving objects: 100% (68831/68831), 9.54 GiB | 46.61 MiB/s, done.\n",
            "Resolving deltas: 100% (288/288), done.\n",
            "Checking out files: 100% (45930/45930), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/deep-learning-project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb7STbfaoq9",
        "outputId": "14db5267-57fb-40bd-9638-3cf76fdacc98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/deep-learning-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Cd4VYQOqpsTQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as mpl_color_map\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4_sxnERZMav"
      },
      "source": [
        "## Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y02InYzsZMaz"
      },
      "outputs": [],
      "source": [
        "class AVADataSet(Dataset):\n",
        "    def __init__(self, csv, img_dir, transform, target_var):\n",
        "        self.img_labels = csv\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_var = target_var\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels.index)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        trans = transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.size(0)==1 else x)\n",
        "        img_name = str(self.img_labels.loc[idx, 'image']) + '.jpg'\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        image = read_image(img_path)\n",
        "        image = trans(image).to(torch.float64)/255\n",
        "        if image.size(0) != 3:\n",
        "            image = image[0:3]\n",
        "\n",
        "        image = self.transform(image)\n",
        "        # get label form csv\n",
        "        label = self.img_labels.loc[idx, self.target_var]\n",
        "        return image, label\n",
        "    def get_name(self, idx):\n",
        "        img_name = str(self.img_labels.loc[idx, 'image']) + '.jpg'\n",
        "        label = self.img_labels.loc[idx, self.target_var]\n",
        "        return img_name, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWYkHMgKZMa0"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdCbFrHCoBc1"
      },
      "outputs": [],
      "source": [
        "AVA = True\n",
        "batch_size = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIBm0w7NoKx3"
      },
      "outputs": [],
      "source": [
        "if AVA == False:\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize([224, 224]),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    dataset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=True, download=True, transform=transform\n",
        "    )\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=False, download=True, transform=transform\n",
        "    )\n",
        "    val_size = 5000\n",
        "    train_size = len(dataset) - val_size\n",
        "    trainset, valset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        valset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch_size, shuffle=True, num_workers=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ5JfW8tVKcf"
      },
      "outputs": [],
      "source": [
        "if AVA == True:\n",
        "    transform = transforms.Compose(\n",
        "        [\n",
        "            #transforms.ToTensor(),\n",
        "            transforms.Resize([224, 224]),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ]\n",
        "    )\n",
        "    data = pd.read_csv(\"datasets/data_class.csv\")\n",
        "\n",
        "    train_data = data.sample(frac=0.75)\n",
        "    rest = data.drop(train_data.index)\n",
        "    val_data = rest.sample(frac=0.5)\n",
        "    test_data = rest.drop(val_data.index)\n",
        "\n",
        "    train_dataset = AVADataSet(\n",
        "        train_data.reset_index(),\n",
        "        \"dataset_class\",\n",
        "        transform=transform,\n",
        "        target_var=\"2class\",\n",
        "    )\n",
        "    val_dataset = AVADataSet(\n",
        "        val_data.reset_index(),\n",
        "        \"dataset_class\",\n",
        "        transform=transform,\n",
        "        target_var=\"2class\",\n",
        "    )\n",
        "    test_dataset = AVADataSet(\n",
        "        test_data.reset_index(),\n",
        "        \"dataset_class\",\n",
        "        transform=transform,\n",
        "        target_var=\"2class\",\n",
        "    )\n",
        "\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        test_dataset, batch_size=batch_size, shuffle=True, num_workers=0\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQoagNzupvvF"
      },
      "outputs": [],
      "source": [
        "class SlotAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    This function is heavily build on the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, dim, iters=3, eps=1e-8, power=2):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.iters = iters\n",
        "        self.eps = eps\n",
        "        self.scale = dim**-0.5\n",
        "        self.power = power\n",
        "\n",
        "        mu = nn.Parameter(torch.randn(1, 1, dim)).expand(1, self.num_classes, -1)\n",
        "        sigma = abs(nn.Parameter(torch.randn(1, 1, dim))).expand(\n",
        "            1, self.num_classes, -1\n",
        "        )\n",
        "        self.initial_slots = nn.Parameter(torch.normal(mu, sigma))\n",
        "\n",
        "        self.FC1 = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "        self.FC2 = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "        self.gru = nn.GRU(dim, dim)\n",
        "\n",
        "    def dot_attention(self, slots, inputs):\n",
        "        dots = torch.einsum(\"bid,bjd->bij\", slots, inputs) * self.scale\n",
        "        div = torch.div(\n",
        "            dots, dots.sum(2).expand_as(dots.permute([2, 0, 1])).permute([1, 2, 0])\n",
        "        )\n",
        "        mult = dots.sum(2).sum(1).expand_as(dots.permute([1, 2, 0])).permute([2, 0, 1])\n",
        "        return div * mult\n",
        "\n",
        "    def save_slot(self, slots):\n",
        "        out = (slots - slots.min()) / (slots.max() - slots.min()) * 255.0\n",
        "        out = out.reshape(\n",
        "            out.shape[:1] + (int(out.size(1) ** 0.5), int(out.size(1) ** 0.5))\n",
        "        )\n",
        "        out = (out.cpu().detach().numpy()).astype(np.uint8)\n",
        "        for i, image in enumerate(out):\n",
        "            image = Image.fromarray(image, mode=\"L\")\n",
        "            image.save(f\"slot_{i:d}.png\")\n",
        "\n",
        "    def forward(self, inputs, inputs_x):\n",
        "        batch, channels, elements = inputs.shape\n",
        "        slots = self.initial_slots.expand(batch, -1, -1)\n",
        "        inputs = self.FC1(inputs)\n",
        "\n",
        "        for _ in range(self.iters):\n",
        "            prev_slots = slots\n",
        "\n",
        "            slots = self.FC2(slots)\n",
        "\n",
        "            dots = self.dot_attention(slots, inputs)\n",
        "            attn = torch.sigmoid(dots)\n",
        "\n",
        "            updates = torch.einsum(\"bjd,bij->bid\", inputs_x, attn)\n",
        "            updates = updates / inputs_x.size(2)\n",
        "\n",
        "            self.gru.flatten_parameters()\n",
        "            slots, _ = self.gru(\n",
        "                updates.reshape(1, -1, elements), prev_slots.reshape(1, -1, elements)\n",
        "            )\n",
        "\n",
        "            slots = slots.reshape(batch, -1, elements)\n",
        "\n",
        "        slots_vis = attn.clone()\n",
        "        slots_vis = slots_vis[0]\n",
        "\n",
        "        self.save_slot(slots_vis)\n",
        "\n",
        "        attn_relu = torch.relu(attn)\n",
        "        loss = torch.sum(attn_relu) / attn.size(0) / attn.size(1) / attn.size(2)\n",
        "        loss = torch.pow(loss, self.power)\n",
        "        output = torch.sum(updates, dim=2, keepdim=False)\n",
        "\n",
        "        return output, loss\n",
        "\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This function is the same as the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nfeats=64, temperature=10000):\n",
        "        super().__init__()\n",
        "        self.nfeats = nfeats\n",
        "        self.temperature = temperature\n",
        "        self.scale = 2 * math.pi\n",
        "\n",
        "    def forward(self, tensor_list):\n",
        "        x = tensor_list\n",
        "        b, c, h, w = x.shape\n",
        "        mask = torch.zeros((b, h, w), dtype=torch.bool, device=x.device)\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "\n",
        "        eps = 1e-6\n",
        "        y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "        x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.nfeats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.nfeats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack(\n",
        "            (pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4\n",
        "        ).flatten(3)\n",
        "        pos_y = torch.stack(\n",
        "            (pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4\n",
        "        ).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos.to(x.dtype)\n",
        "\n",
        "\n",
        "class Identical(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identical, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "\n",
        "class SlotModel(nn.Module):\n",
        "    \"\"\"\n",
        "    This function is heavily build on the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, hidden_dim, input_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.conv1x1 = nn.Conv2d(512, hidden_dim, kernel_size=(1, 1), stride=(1, 1))\n",
        "        N_steps = hidden_dim // 2\n",
        "        self.position_emb = PositionEmbeddingSine(N_steps)\n",
        "        self.slot = SlotAttention(num_classes, hidden_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), *self.input_size)\n",
        "        x = self.conv1x1(x)\n",
        "        x = torch.relu(x)\n",
        "        pe = self.position_emb(x)\n",
        "        x_pe = x + pe\n",
        "\n",
        "        batch, channel, r, c = x.shape\n",
        "        x = x.reshape((batch, channel, -1)).permute((0, 2, 1))\n",
        "\n",
        "        x_pe = x_pe.reshape((batch, channel, -1)).permute((0, 2, 1))\n",
        "        x, attn_loss = self.slot(x_pe, x)\n",
        "\n",
        "        return x, attn_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTs-JiwNawBw"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, slot, lambda_value):\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    for images, targets in dataloader:\n",
        "        images, targets = images.to(device, dtype=torch.float), targets.to(device)\n",
        "        if slot:\n",
        "            output, attn_loss = model(images)\n",
        "            n_loss = criterion(output, targets)\n",
        "            loss = n_loss + lambda_value * attn_loss\n",
        "        else:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(output, targets)\n",
        "\n",
        "        topv, topi = torch.topk(output, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(topi.view(topi.size(0)) == targets)\n",
        "\n",
        "    loss = running_loss / len(dataloader.dataset)\n",
        "    acc = running_corrects.double() / len(dataloader.dataset)\n",
        "    return loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ICrF62rwpv18"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloaders, nEpochs, optimizer, device, slot, lambda_value=None):\n",
        "    l_acc = {\"train\": [], \"val\": []}\n",
        "    l_loss = {\"train\": [], \"val\": []}\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    #print(\"Initial performance\")\n",
        "    #for phase in [\"train\", \"val\"]:\n",
        "    #    init_loss, init_acc = evaluate(\n",
        "    #        model, dataloaders[phase], criterion, slot, lambda_value\n",
        "    #    )\n",
        "    #    l_acc[phase].append(init_acc.item())\n",
        "    #    l_loss[phase].append(init_loss)\n",
        "#\n",
        "    #    print(f\"{phase}: Loss: {init_loss}, Accuracy: {init_acc}\")\n",
        "\n",
        "    print(\"Start training\")\n",
        "    best_loss = 100\n",
        "    for epoch in range(nEpochs):\n",
        "        print(\"Epoch: \", epoch)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for images, targets in dataloaders[phase]:\n",
        "                if phase == \"train\":\n",
        "                    torch.set_grad_enabled(True)\n",
        "                    model.train()\n",
        "\n",
        "                else:\n",
        "                    torch.set_grad_enabled(False)\n",
        "                    model.eval()\n",
        "\n",
        "                images = images.float()\n",
        "                images = images.to(device)\n",
        "                targets = targets.to(device)\n",
        "                if slot:\n",
        "                    output, attn_loss = model(images)\n",
        "                    n_loss = criterion(output, targets)\n",
        "                    loss = n_loss + lambda_value * attn_loss\n",
        "                else:\n",
        "                    output = model(images)\n",
        "                    loss = criterion(output, targets)\n",
        "\n",
        "                if phase == \"train\":\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                topv, topi = torch.topk(output, 1)\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "                running_corrects += torch.sum(topi.view(topi.size(0)) == targets)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "            l_loss[phase].append(epoch_loss)\n",
        "            l_acc[phase].append(epoch_acc.item())\n",
        "\n",
        "            if best_loss > epoch_loss and phase == \"val\":\n",
        "                best_loss = epoch_loss\n",
        "                end_model = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            print(f\"{phase}: Loss: {epoch_loss}, Accuracy: {epoch_acc}\")\n",
        "\n",
        "    model.load_state_dict(end_model)\n",
        "    return model, l_loss, l_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tef7oes6XUq",
        "outputId": "0ba454a9-1f85-4500-9816-8d1b4987fb00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters trained:\n",
            "layer4.0.conv1.weight\n",
            "layer4.0.bn1.weight\n",
            "layer4.0.bn1.bias\n",
            "layer4.0.conv2.weight\n",
            "layer4.0.bn2.weight\n",
            "layer4.0.bn2.bias\n",
            "layer4.0.downsample.0.weight\n",
            "layer4.0.downsample.1.weight\n",
            "layer4.0.downsample.1.bias\n",
            "layer4.1.conv1.weight\n",
            "layer4.1.bn1.weight\n",
            "layer4.1.bn1.bias\n",
            "layer4.1.conv2.weight\n",
            "layer4.1.bn2.weight\n",
            "layer4.1.bn2.bias\n",
            "fc.conv1x1.weight\n",
            "fc.conv1x1.bias\n",
            "fc.slot.initial_slots\n",
            "fc.slot.FC1.0.weight\n",
            "fc.slot.FC1.0.bias\n",
            "fc.slot.FC1.2.weight\n",
            "fc.slot.FC1.2.bias\n",
            "fc.slot.FC1.4.weight\n",
            "fc.slot.FC1.4.bias\n",
            "fc.slot.FC2.0.weight\n",
            "fc.slot.FC2.0.bias\n",
            "fc.slot.FC2.2.weight\n",
            "fc.slot.FC2.2.bias\n",
            "fc.slot.FC2.4.weight\n",
            "fc.slot.FC2.4.bias\n",
            "fc.slot.gru.weight_ih_l0\n",
            "fc.slot.gru.weight_hh_l0\n",
            "fc.slot.gru.bias_ih_l0\n",
            "fc.slot.gru.bias_hh_l0\n",
            "Start training\n",
            "Epoch:  0\n",
            "train: Loss: 0.740269065547222, Accuracy: 0.5816101301115242\n",
            "val: Loss: 0.6132134076359796, Accuracy: 0.639944231439526\n",
            "Epoch:  1\n",
            "train: Loss: 0.5509014879197547, Accuracy: 0.7286245353159851\n",
            "val: Loss: 0.4986742101466876, Accuracy: 0.7640292784942488\n",
            "Epoch:  2\n",
            "train: Loss: 0.4517715402210623, Accuracy: 0.7886849442379181\n",
            "val: Loss: 0.46481365535723784, Accuracy: 0.7845939351690484\n",
            "Epoch:  3\n",
            "train: Loss: 0.39660867861364746, Accuracy: 0.8192669609665427\n",
            "val: Loss: 0.4665748299871766, Accuracy: 0.7866852561868247\n",
            "Epoch:  4\n",
            "train: Loss: 0.3404120405890103, Accuracy: 0.8500232342007434\n",
            "val: Loss: 0.4880851656146563, Accuracy: 0.7845939351690484\n",
            "Epoch:  5\n",
            "train: Loss: 0.2766481883024111, Accuracy: 0.882493029739777\n",
            "val: Loss: 0.5399120337209372, Accuracy: 0.780411293133496\n",
            "Epoch:  6\n",
            "train: Loss: 0.20666774708282576, Accuracy: 0.9160664498141263\n",
            "val: Loss: 0.6699333255321732, Accuracy: 0.7793656326246079\n",
            "Epoch:  7\n",
            "train: Loss: 0.14290100793037425, Accuracy: 0.946183782527881\n",
            "val: Loss: 0.8142457503232029, Accuracy: 0.7713489020564657\n"
          ]
        }
      ],
      "source": [
        "if AVA == False:\n",
        "    nclass = 10\n",
        "else:\n",
        "    nclass = 2\n",
        "\n",
        "dataloaders = {\"train\": trainloader, \"val\": valloader}\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "num_fts = resnet.fc.in_features\n",
        "\n",
        "for name, param in resnet.named_parameters():\n",
        "    if \"layer4\" not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "resnet.avgpool = Identical()\n",
        "resnet.fc = SlotModel(num_classes=nclass, hidden_dim=128, input_size=(512,7,7))\n",
        "resnet.to(device)\n",
        "\n",
        "print('Parameters trained:')\n",
        "params_to_update = []\n",
        "for name, param in resnet.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)\n",
        "        params_to_update.append(param)\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=1e-5)        \n",
        "model, h_loss, h_accuracy = train(model=resnet, dataloaders=dataloaders, nEpochs=8, optimizer=optimizer, device=device, slot=True, lambda_value=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qZC9A_dTv46j"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lc35ndqZGikb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97a425ec-31e3-4783-df34-b98699390069"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4818553708741673, tensor(0.7778, device='cuda:0', dtype=torch.float64))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "evaluate(model, testloader, criterion, slot=True, lambda_value=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6Bj-ksv4Oqp2"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), f'Slot_AVA_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "04Mb--aPHJVU"
      },
      "outputs": [],
      "source": [
        "def apply_colormap_on_image(org_im, activation, colormap_name):\n",
        "    \"\"\"\n",
        "    This function is the same as the function proposed in the article \"SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition\", 2021\n",
        "    by Liangzhi Li and Bowen Wang and Manisha Verma and Yuta Nakashima and Ryo Kawasaki and Hajime Nagahara\n",
        "    from the book \"IEEE International Conference on Computer Vision (ICCV)\"\n",
        "    The original function can be found on https://github.com/wbw520/scouter\n",
        "    \"\"\"\n",
        "    color_map = mpl_color_map.get_cmap(colormap_name)\n",
        "    no_trans_heatmap = color_map(activation)\n",
        "    heatmap = copy.copy(no_trans_heatmap)\n",
        "    heatmap[:, :, 3] = 0.4\n",
        "    heatmap = Image.fromarray((heatmap * 255).astype(np.uint8))\n",
        "    no_trans_heatmap = Image.fromarray((no_trans_heatmap * 255).astype(np.uint8))\n",
        "    heatmap_on_image = Image.new(\"RGBA\", (224,224))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, org_im.convert(\"RGBA\"))\n",
        "    heatmap_on_image = Image.alpha_composite(heatmap_on_image, heatmap)\n",
        "    return no_trans_heatmap, heatmap_on_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3reVc3Z1Wqcn"
      },
      "outputs": [],
      "source": [
        "def eval_attn(model, imageid, device):\n",
        "    trans = transforms.Compose([transforms.Resize([224, 224]),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "    model.eval()\n",
        "    image = read_image(f'dataset_class/{imageid}')\n",
        "    image = image.to(torch.float64)/255\n",
        "    image = trans(image)\n",
        "    image = image.float()\n",
        "    image = image.to(device)\n",
        "    out, att = model(image.view(1,3,224,224))\n",
        "    print(f'prediction: {out.cpu().numpy()[0]}')\n",
        "    for id in range(2):\n",
        "        image_raw = Image.open(f'dataset_class/{imageid}').convert('RGB')\n",
        "        image_raw = image_raw.resize((224,224), resample=Image.BILINEAR)\n",
        "        slot_image = np.array(Image.open(f'slot_{id}.png').resize((224,224), resample=Image.BILINEAR), dtype=np.uint8)\n",
        "        heatmap_only, heatmap_on_image = apply_colormap_on_image(image_raw, slot_image, 'jet')\n",
        "        heatmap_on_image.save(f'slot_mask_{id}.png')   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "j9uva7Thajil",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2599e89-cfdb-4b0b-eb5d-061ec640e69a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction: [1.9865899 3.2606378]\n"
          ]
        }
      ],
      "source": [
        "#test_dataset\n",
        "eval_attn(model, '937378.jpg', device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha6HJVDh29jR"
      },
      "outputs": [],
      "source": [
        "#classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(300,400):\n",
        "  k = test_dataset.get_name(i)\n",
        "  if k[1] == 1:\n",
        "    print(k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRJOluhwuSVu",
        "outputId": "e7a49490-0a30-4ed3-c187-28ea51b88c35"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('622433.jpg', 1)\n",
            "('623493.jpg', 1)\n",
            "('622057.jpg', 1)\n",
            "('622776.jpg', 1)\n",
            "('623340.jpg', 1)\n",
            "('623862.jpg', 1)\n",
            "('830969.jpg', 1)\n",
            "('11970.jpg', 1)\n",
            "('11875.jpg', 1)\n",
            "('11926.jpg', 1)\n",
            "('636175.jpg', 1)\n",
            "('636181.jpg', 1)\n",
            "('635159.jpg', 1)\n",
            "('634542.jpg', 1)\n",
            "('635164.jpg', 1)\n",
            "('636213.jpg', 1)\n",
            "('635651.jpg', 1)\n",
            "('634733.jpg', 1)\n",
            "('635834.jpg', 1)\n",
            "('635514.jpg', 1)\n",
            "('848911.jpg', 1)\n",
            "('847136.jpg', 1)\n",
            "('848636.jpg', 1)\n",
            "('847541.jpg', 1)\n",
            "('847039.jpg', 1)\n",
            "('848905.jpg', 1)\n",
            "('847305.jpg', 1)\n",
            "('848495.jpg', 1)\n",
            "('848916.jpg', 1)\n",
            "('931054.jpg', 1)\n",
            "('935424.jpg', 1)\n",
            "('934122.jpg', 1)\n",
            "('935570.jpg', 1)\n",
            "('933409.jpg', 1)\n",
            "('935362.jpg', 1)\n",
            "('932028.jpg', 1)\n",
            "('638688.jpg', 1)\n",
            "('638536.jpg', 1)\n",
            "('638038.jpg', 1)\n",
            "('638861.jpg', 1)\n",
            "('638267.jpg', 1)\n",
            "('638904.jpg', 1)\n",
            "('638987.jpg', 1)\n",
            "('638209.jpg', 1)\n",
            "('637949.jpg', 1)\n",
            "('927291.jpg', 1)\n",
            "('861319.jpg', 1)\n",
            "('861126.jpg', 1)\n",
            "('60212.jpg', 1)\n",
            "('60082.jpg', 1)\n",
            "('60223.jpg', 1)\n",
            "('651710.jpg', 1)\n",
            "('651187.jpg', 1)\n",
            "('651923.jpg', 1)\n",
            "('651740.jpg', 1)\n",
            "('651799.jpg', 1)\n",
            "('873887.jpg', 1)\n",
            "('874261.jpg', 1)\n",
            "('872811.jpg', 1)\n",
            "('873358.jpg', 1)\n",
            "('873987.jpg', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1,2)\n",
        "#plt.plot\n",
        "ax[0].plot(h_loss[\"train\"], label = \"train loss\")\n",
        "ax[0].plot(h_loss[\"val\"], label = \"validation loss\")\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"epoch\")\n",
        "ax[0].set_ylabel(\"loss\") \n",
        "ax[1].plot(h_accuracy[\"train\"], label = \"train accuracy\")\n",
        "ax[1].plot(h_accuracy[\"val\"], label = \"validation accuracy\")\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"epoch\")\n",
        "ax[1].set_ylabel(\"Accuracy\") \n",
        "fig.tight_layout()\n",
        "fig.set_figwidth(6)\n",
        "fig.set_figheight(2)\n",
        "fig.savefig(\"Loss_Acc\", format=\"png\")"
      ],
      "metadata": {
        "id": "irGa22fe54Qy",
        "outputId": "d6797c61-e47f-4c54-963b-7474f00936e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        }
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x144 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h_loss"
      ],
      "metadata": {
        "id": "wqgrSq8m6DQc",
        "outputId": "7f8a708e-ed3b-4d5b-87f6-8f97605c94ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': [0.740269065547222,\n",
              "  0.5509014879197547,\n",
              "  0.4517715402210623,\n",
              "  0.39660867861364746,\n",
              "  0.3404120405890103,\n",
              "  0.2766481883024111,\n",
              "  0.20666774708282576,\n",
              "  0.14290100793037425],\n",
              " 'val': [0.6132134076359796,\n",
              "  0.4986742101466876,\n",
              "  0.46481365535723784,\n",
              "  0.4665748299871766,\n",
              "  0.4880851656146563,\n",
              "  0.5399120337209372,\n",
              "  0.6699333255321732,\n",
              "  0.8142457503232029]}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}