{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/louisemoelgaard/Dataproject/blob/main/slot_virker_upconvolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "Cd4VYQOqpsTQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "import math\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SlotAttention(nn.Module):\n",
        "    def __init__(self, num_classes, slots_per_class, dim, iters=3, eps=1e-8, vis=True, vis_id=0, loss_status=1, power=1, to_k_layer=1):\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.slots_per_class = slots_per_class\n",
        "        self.num_slots = num_classes * slots_per_class\n",
        "        self.iters = iters\n",
        "        self.eps = eps\n",
        "        self.scale = dim ** -0.5\n",
        "        self.loss_status = loss_status\n",
        "\n",
        "        slots_mu = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        slots_sigma = abs(nn.Parameter(torch.randn(1, 1, dim)))\n",
        "\n",
        "        mu = slots_mu.expand(1, self.num_slots, -1)\n",
        "        sigma = slots_sigma.expand(1, self.num_slots, -1)\n",
        "        self.initial_slots = nn.Parameter(torch.normal(mu, sigma))\n",
        "\n",
        "        self.to_q = nn.Sequential(\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "        to_k_layer_list = [nn.Linear(dim, dim)]\n",
        "        for to_k_layer_id in range(1, to_k_layer):\n",
        "            to_k_layer_list.append(nn.ReLU(inplace=True))\n",
        "            to_k_layer_list.append(nn.Linear(dim, dim))\n",
        "        \n",
        "        self.to_k = nn.Sequential(\n",
        "            *to_k_layer_list\n",
        "        )\n",
        "        self.gru = nn.GRU(dim, dim)\n",
        "\n",
        "        self.vis = vis\n",
        "        self.vis_id = vis_id\n",
        "        self.power = power\n",
        "\n",
        "    def forward(self, inputs, inputs_x):\n",
        "        b, n, d = inputs.shape\n",
        "        slots = self.initial_slots.expand(b, -1, -1)\n",
        "        k, v = self.to_k(inputs), inputs\n",
        "\n",
        "        for _ in range(self.iters):\n",
        "            slots_prev = slots\n",
        "\n",
        "            # q = self.to_q(slots)\n",
        "            q = slots\n",
        "\n",
        "            dots = torch.einsum('bid,bjd->bij', q, k) * self.scale\n",
        "            dots = torch.div(dots, dots.sum(2).expand_as(dots.permute([2,0,1])).permute([1,2,0])) * dots.sum(2).sum(1).expand_as(dots.permute([1,2,0])).permute([2,0,1])# * 10\n",
        "            attn = torch.sigmoid(dots)\n",
        "            updates = torch.einsum('bjd,bij->bid', inputs_x, attn)\n",
        "            updates = updates / inputs_x.size(2)\n",
        "            self.gru.flatten_parameters()\n",
        "            slots, _ = self.gru(\n",
        "                updates.reshape(1, -1, d),\n",
        "                slots_prev.reshape(1, -1, d)\n",
        "            )\n",
        "\n",
        "            slots = slots.reshape(b, -1, d)\n",
        "\n",
        "            if self.vis:\n",
        "                slots_vis = attn.clone()\n",
        "\n",
        "        if self.vis:\n",
        "            if self.slots_per_class > 1:\n",
        "                new_slots_vis = torch.zeros((slots_vis.size(0), self.num_classes, slots_vis.size(-1)))\n",
        "                for slot_class in range(self.num_classes):\n",
        "                    new_slots_vis[:, slot_class] = torch.sum(torch.cat([slots_vis[:, self.slots_per_class*slot_class: self.slots_per_class*(slot_class+1)]], dim=1), dim=1, keepdim=False)\n",
        "                slots_vis = new_slots_vis.to(updates.device)\n",
        "\n",
        "            slots_vis = slots_vis[self.vis_id]\n",
        "            slots_vis = ((slots_vis - slots_vis.min()) / (slots_vis.max()-slots_vis.min()) * 255.).reshape(slots_vis.shape[:1]+(int(slots_vis.size(1)**0.5), int(slots_vis.size(1)**0.5)))\n",
        "            slots_vis = (slots_vis.cpu().detach().numpy()).astype(np.uint8)\n",
        "            for id, image in enumerate(slots_vis):\n",
        "                image = Image.fromarray(image, mode='L')\n",
        "                image.save(f'slot_{id:d}.png')\n",
        "            #print(self.loss_status*torch.sum(attn.clone(), dim=2, keepdim=False))\n",
        "            #print(self.loss_status*torch.sum(updates.clone(), dim=2, keepdim=False))\n",
        "\n",
        "        if self.slots_per_class > 1:\n",
        "            new_updates = torch.zeros((updates.size(0), self.num_classes, updates.size(-1)))\n",
        "            for slot_class in range(self.num_classes):\n",
        "                new_updates[:, slot_class] = torch.sum(updates[:, self.slots_per_class*slot_class: self.slots_per_class*(slot_class+1)], dim=1, keepdim=False)\n",
        "            updates = new_updates.to(updates.device)\n",
        "\n",
        "        attn_relu = torch.relu(attn)\n",
        "        slot_loss = torch.sum(attn_relu) / attn.size(0) / attn.size(1) / attn.size(2)# * self.slots_per_class\n",
        "\n",
        "        return self.loss_status*torch.sum(updates, dim=2, keepdim=False), torch.pow(slot_loss, self.power)\n",
        "\n",
        "class PositionEmbeddingSine(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a more standard version of the position embedding, very similar to the one\n",
        "    used by the Attention is all you need paper, generalized to work on images.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_pos_feats=64, temperature=10000, normalize=False, scale=None):\n",
        "        super().__init__()\n",
        "        self.num_pos_feats = num_pos_feats\n",
        "        self.temperature = temperature\n",
        "        self.normalize = normalize\n",
        "        if scale is not None and normalize is False:\n",
        "            raise ValueError(\"normalize should be True if scale is passed\")\n",
        "        if scale is None:\n",
        "            scale = 2 * math.pi\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, tensor_list):\n",
        "        x = tensor_list\n",
        "        b, c, h, w = x.shape\n",
        "        mask = torch.zeros((b, h, w), dtype=torch.bool, device=x.device)\n",
        "        not_mask = ~mask\n",
        "        y_embed = not_mask.cumsum(1, dtype=torch.float32)\n",
        "        x_embed = not_mask.cumsum(2, dtype=torch.float32)\n",
        "        if self.normalize:\n",
        "            eps = 1e-6\n",
        "            y_embed = y_embed / (y_embed[:, -1:, :] + eps) * self.scale\n",
        "            x_embed = x_embed / (x_embed[:, :, -1:] + eps) * self.scale\n",
        "\n",
        "        dim_t = torch.arange(self.num_pos_feats, dtype=torch.float32, device=x.device)\n",
        "        dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n",
        "\n",
        "        pos_x = x_embed[:, :, :, None] / dim_t\n",
        "        pos_y = y_embed[:, :, :, None] / dim_t\n",
        "        pos_x = torch.stack((pos_x[:, :, :, 0::2].sin(), pos_x[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos_y = torch.stack((pos_y[:, :, :, 0::2].sin(), pos_y[:, :, :, 1::2].cos()), dim=4).flatten(3)\n",
        "        pos = torch.cat((pos_y, pos_x), dim=3).permute(0, 3, 1, 2)\n",
        "        return pos.to(x.dtype)\n",
        "\n",
        "class Identical(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identical, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "class SlotModel(nn.Module):\n",
        "    def __init__(self, num_classes, slots_per_class, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        #self.model = pre_model\n",
        "\n",
        "            \n",
        "        self.slots_per_class = slots_per_class\n",
        "        self.conv1x1 = nn.Conv2d(256, hidden_dim, kernel_size=(1, 1), stride=(1, 1))\n",
        "           \n",
        "        self.slot = SlotAttention(num_classes, self.slots_per_class, hidden_dim)\n",
        "        \n",
        "        \n",
        "        N_steps = hidden_dim // 2\n",
        "        self.position_emb = PositionEmbeddingSine(N_steps, normalize=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.model(x)\n",
        "        \n",
        "        x = self.conv1x1(x.view(x.size(0), 256, 14, 14))\n",
        "        x = torch.relu(x)\n",
        "        pe = self.position_emb(x)\n",
        "        x_pe = x + pe\n",
        "        \n",
        "        b, n, r, c = x.shape\n",
        "        x = x.reshape((b, n, -1)).permute((0, 2, 1))\n",
        "        x_pe = x_pe.reshape((b, n, -1)).permute((0, 2, 1))\n",
        "        x, attn_loss = self.slot(x_pe, x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        \n",
        "        return output, attn_loss\n",
        "        "
      ],
      "metadata": {
        "id": "RQoagNzupvvF"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rc8RdlbYFIZx"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, nEpochs,optimizer, device):\n",
        "    acc_history=[]\n",
        "    loss_history=[]\n",
        "    lambda_value = 1.\n",
        "    for epoch in range(nEpochs):\n",
        "        print(epoch)\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        ## Train\n",
        "        model.train()\n",
        "        cur_loss = 0\n",
        "       \n",
        "        for images, targets in dataloader:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            output, attn_loss = model(images) \n",
        "            \n",
        "            #print(output.size(), targets.size())\n",
        "            \n",
        "            loss = F.nll_loss(output, targets) + lambda_value * attn_loss\n",
        "            \n",
        "            logits = output\n",
        "            loss_list =  [loss, F.nll_loss(output, targets), attn_loss]# Forward pass\n",
        "            loss = loss_list[0]\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            topv, topi = torch.topk(output,1)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            running_corrects += torch.sum(topi.view(topi.size(0)) == targets)\n",
        "            \n",
        "        epoch_loss = running_loss / len(dataloader.dataset)\n",
        "        epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "        acc_history.append(epoch_acc)\n",
        "        loss_history.append(epoch_loss)\n",
        "    \n",
        "    return [acc_history, loss_history]"
      ],
      "metadata": {
        "id": "ICrF62rwpv18"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchvision.datasets.CIFAR10(root='./data', download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGcZMVFQpv5P",
        "outputId": "0eef3b5b-9c4f-4f3a-9379-15466d97f42f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset CIFAR10\n",
              "    Number of datapoints: 50000\n",
              "    Root location: ./data\n",
              "    Split: Train"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Transforms are common image transformations, that can be stacked and used for preprocessing images. \n",
        "# Here, our preprocessing consists of converting the data to torch tensors and normalizing the data \n",
        "# with 0.5 mean and 0.5 std. diviation for all three channels\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Resize([224,224]),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 150\n",
        "\n",
        "\n",
        "# Set up the training set. The data-set helper function already implemented a data-split\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "# Set up the test set. The data-set helper function already implemented a data-split\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# The CIFAR-10 Classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFvAN9mOpv82",
        "outputId": "b80f0ae4-7150-4c07-d36f-eb5bac9f66cc"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "resnet.avgpool = resnet.avgpool = nn.ConvTranspose2d(512, out_channels = 256, kernel_size = (10,10), stride = (1,1), padding=(1, 1), bias=False)\n",
        "resnet.fc = SlotModel(num_classes=10, slots_per_class=1, hidden_dim=512)\n",
        "\n",
        "resnet.to(device)\n",
        "\n",
        "\n",
        "for name, param in resnet.named_parameters():\n",
        "    if \"layer4\" not in name:  \n",
        "        param.requires_grad = False\n",
        "\n",
        "params_to_update = []\n",
        "for name, param in resnet.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        params_to_update.append(param)\n",
        "\n",
        "optimizer = torch.optim.Adam(params_to_update, lr=1e-4)        \n",
        "\n",
        "rs = train(model=resnet, dataloader=trainloader, nEpochs=5, optimizer=optimizer, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "7Tef7oes6XUq",
        "outputId": "ee6131d1-6cb2-4da7-8997-a3d35d61a3e4"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-d1a6591d731e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_to_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnEpochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-97-652d7251a4a8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, nEpochs, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#print(output.size(), targets.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-5b3f6fb1605f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mx_pe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_pe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-96-5b3f6fb1605f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, inputs_x)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mslot_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_relu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# * self.slots_per_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_status\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslot_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPositionEmbeddingSine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8xv2eKsPn5sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = 'cifar_net.pth'\n",
        "torch.save(resnet.state_dict(), MODEL_PATH)"
      ],
      "metadata": {
        "id": "Vg67AcFT9-Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rs"
      ],
      "metadata": {
        "id": "9jIn-5yy7fJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "#resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "#Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "#resnet.avgpool = nn.ConvTranspose2d(512, out_channels = 256, kernel_size = (6,6), stride = (1,1), padding=(1, 1), bias=False)\n",
        "\n",
        "#resnet.fc = Identical()\n",
        "\n",
        "#resnet.fc = SlotModel(num_classes=10, slots_per_class=1, hidden_dim=512)\n",
        "\n",
        "#resnet.to(device)\n",
        "\n",
        "\n",
        "#for name, param in resnet.named_parameters():\n",
        "#    if \"layer4\" not in name:  \n",
        "#        param.requires_grad = False\n",
        "\n",
        "#params_to_update = []\n",
        "#for name, param in resnet.named_parameters():\n",
        "   # if param.requires_grad:\n",
        " #       params_to_update.append(param)\n",
        "#input = torch.rand(1,3,224,224).to(device)\n",
        "\n",
        "#resnet(input).size()"
      ],
      "metadata": {
        "id": "_Z3WMlmzM6uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "def eval_attn(model, images, targets):\n",
        "  model.eval()\n",
        "  for image, target in zip(images, targets):\n",
        "    image = image.view(1,image.size(0),image.size(1),image.size(2))\n",
        "\n",
        "    out, att = model(image)\n",
        "    out = out.cpu()\n",
        "    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "    print(f'True class: {classes[target.item()]}')\n",
        "    print(f'Predicted class: {classes[np.argmax(out.detach())]}')\n",
        "    image = torchvision.utils.make_grid(image.cpu())\n",
        "    img = image / 2 + 0.5     # unnormalize\n",
        "    npimg = np.transpose(img.numpy(), (1, 2, 0))\n",
        "\n",
        "    fig, axs = plt.subplots(2, 5)\n",
        "    axs = axs.ravel()\n",
        "    extent = 0,224,0,224\n",
        "    for i in range(10):\n",
        "      img = mpimg.imread(f'slot_{i}.png')\n",
        "      axs[i].imshow(npimg, extent=extent)\n",
        "      axs[i].imshow(img, alpha=0.5, extent=extent)\n",
        "      axs[i].set_title(f'Why {classes[i]}')\n",
        "    break\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "\n"
      ],
      "metadata": {
        "id": "IeTjigTgYLhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, target = dataiter.next()\n",
        "image = image.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "\n",
        "eval_attn(resnet, image, target)"
      ],
      "metadata": {
        "id": "kMCyemhzZkGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}